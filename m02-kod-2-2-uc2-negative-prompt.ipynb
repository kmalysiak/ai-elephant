{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers diffusers accelerate safetensors --upgrade ","metadata":{"execution":{"iopub.status.busy":"2024-09-06T13:50:23.982795Z","iopub.execute_input":"2024-09-06T13:50:23.983174Z","iopub.status.idle":"2024-09-06T13:50:37.346731Z","shell.execute_reply.started":"2024-09-06T13:50:23.983135Z","shell.execute_reply":"2024-09-06T13:50:37.345420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This command is installing or upgrading several Python packages using pip, the Python package installer. Let me break it down:\n\n1. `!pip install`: This is the basic pip install command. The `!` at the beginning is often used in Jupyter notebooks to run shell commands.\n\n2. `-q`: This flag stands for \"quiet\". It reduces the output verbosity of pip, showing only important messages.\n\n3. The packages being installed or upgraded are:\n   - `transformers`: A popular library for natural language processing tasks.\n   - `diffusers`: A library for state-of-the-art diffusion models in computer vision.\n   - `accelerate`: A library to easily write distributed and efficient PyTorch code.\n   - `safetensors`: A library for handling tensors (multidimensional arrays) with some added safety features.\n\n4. `--upgrade`: This flag tells pip to upgrade these packages to their latest versions if they're already installed.\n\n","metadata":{}},{"cell_type":"code","source":"from diffusers import StableDiffusionXLPipeline, AutoencoderKL\nimport torch\nimport itertools\nimport cv2\nimport pandas as pd\nfrom random import sample\nfrom PIL import Image","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-09-06T13:50:37.348957Z","iopub.execute_input":"2024-09-06T13:50:37.349332Z","iopub.status.idle":"2024-09-06T13:50:44.510369Z","shell.execute_reply.started":"2024-09-06T13:50:37.349276Z","shell.execute_reply":"2024-09-06T13:50:44.509366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```python\nfrom diffusers import StableDiffusionXLPipeline, AutoencoderKL\n```\nThis line imports two specific classes from the `diffusers` library:\n- `StableDiffusionXLPipeline`: This is likely a pipeline for the Stable Diffusion XL model, which is used for generating high-quality images from text descriptions.\n- `AutoencoderKL`: This is probably an autoencoder model, possibly used for image compression or feature extraction.\n\n```python\nimport torch\n```\nThis imports PyTorch, a popular deep learning framework.\n\n```python\nimport itertools\n```\nThis imports the `itertools` module, which provides various functions for working with iterators efficiently.\n\n```python\nimport cv2\n```\nThis imports OpenCV (cv2), a library for computer vision tasks like image and video processing.\n\n```python\nimport pandas as pd\n```\nThis imports the pandas library (aliased as `pd`), which is used for data manipulation and analysis.\n\n```python\nfrom random import sample\n```\nThis imports the `sample` function from Python's `random` module, which is used for random sampling from a sequence.\n\n```python\nfrom PIL import Image\n```\nThis imports the `Image` module from the Python Imaging Library (PIL), which is used for opening, manipulating, and saving various image file formats.\n\nOverall, this set of imports suggests that the code is likely part of a project involving:\n1. Image generation (using Stable Diffusion XL)\n2. Image processing and analysis\n3. Data handling and manipulation\n4. Possibly some random sampling or selection of images\n","metadata":{"execution":{"iopub.status.busy":"2024-09-06T21:05:59.156307Z","iopub.execute_input":"2024-09-06T21:05:59.156763Z","iopub.status.idle":"2024-09-06T21:05:59.185191Z","shell.execute_reply.started":"2024-09-06T21:05:59.156720Z","shell.execute_reply":"2024-09-06T21:05:59.183565Z"}}},{"cell_type":"code","source":"class CFG:\n    model = \"stabilityai/stable-diffusion-xl-base-1.0\"\n    infsteps = 10\n    howmany = 1","metadata":{"execution":{"iopub.status.busy":"2024-09-06T13:50:44.511610Z","iopub.execute_input":"2024-09-06T13:50:44.512138Z","iopub.status.idle":"2024-09-06T13:50:44.516407Z","shell.execute_reply.started":"2024-09-06T13:50:44.512103Z","shell.execute_reply":"2024-09-06T13:50:44.515510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function - taken from https://www.datacamp.com/tutorial/fine-tuning-stable-diffusion-xl-with-dreambooth-and-lora\n\ndef image_grid(imgs, rows, cols, resize= 512):\n    assert len(imgs) == rows * cols\n\n    if resize is not None:\n        imgs = [img.resize((resize, resize)) for img in imgs]\n\n    w, h = imgs[0].size\n    grid_w, grid_h = cols * w, rows * h\n    grid = Image.new(\"RGB\", size=(grid_w, grid_h))\n\n    for i, img in enumerate(imgs):\n        x = i % cols * w\n        y = i // cols * h\n        grid.paste(img, box=(x, y))\n\n    return grid","metadata":{"execution":{"iopub.status.busy":"2024-09-06T13:50:44.517763Z","iopub.execute_input":"2024-09-06T13:50:44.518530Z","iopub.status.idle":"2024-09-06T13:50:44.526307Z","shell.execute_reply.started":"2024-09-06T13:50:44.518474Z","shell.execute_reply":"2024-09-06T13:50:44.525487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This code defines a helper function called `image_grid` that creates a grid of images. Let's break it down:\n\n```python\ndef image_grid(imgs, rows, cols, resize=512):\n```\nThis line defines the function with four parameters:\n- `imgs`: A list of images\n- `rows`: Number of rows in the grid\n- `cols`: Number of columns in the grid\n- `resize`: Optional parameter to resize images (default is 512 pixels)\n\n```python\nassert len(imgs) == rows * cols\n```\nThis assertion ensures that the number of images matches the specified grid size.\n\n```python\nif resize is not None:\n    imgs = [img.resize((resize, resize)) for img in imgs]\n```\nIf a resize value is provided, this resizes all images to the specified dimensions.\n\n```python\nw, h = imgs[0].size\ngrid_w, grid_h = cols * w, rows * h\n```\nThis calculates the width and height of a single image and the total grid dimensions.\n\n```python\ngrid = Image.new(\"RGB\", size=(grid_w, grid_h))\n```\nCreates a new blank image with the calculated grid dimensions.\n\n```python\nfor i, img in enumerate(imgs):\n    x = i % cols * w\n    y = i // cols * h\n    grid.paste(img, box=(x, y))\n```\nThis loop pastes each image into its correct position in the grid:\n- `i % cols * w` calculates the x-coordinate\n- `i // cols * h` calculates the y-coordinate\n\n```python\nreturn grid\n```\nFinally, the function returns the completed image grid.\n\nThis function is useful for visualizing multiple images in a grid layout, which is common in machine learning projects, especially those involving image generation or processing. The comment mentions it's taken from a DataCamp tutorial on fine-tuning Stable Diffusion XL, suggesting it's part of a larger project involving image generation or manipulation.\n","metadata":{}},{"cell_type":"code","source":"# initialize the pipe\nvae = AutoencoderKL.from_pretrained(\n    \"madebyollin/sdxl-vae-fp16-fix\", \n    torch_dtype=torch.float16\n)\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-09-06T13:50:44.528973Z","iopub.execute_input":"2024-09-06T13:50:44.529363Z","iopub.status.idle":"2024-09-06T13:50:45.212848Z","shell.execute_reply.started":"2024-09-06T13:50:44.529324Z","shell.execute_reply":"2024-09-06T13:50:45.211890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This code is initializing a component of a machine learning pipeline, specifically a Variational Autoencoder (VAE) for use with a Stable Diffusion XL model. Let's break it down:\n\n```python\nvae = AutoencoderKL.from_pretrained(\n    \"madebyollin/sdxl-vae-fp16-fix\", \n    torch_dtype=torch.float16\n)\n```\n\n1. `AutoencoderKL`: This is the class we imported earlier from the `diffusers` library. It represents a type of VAE (Variational Autoencoder) that uses Kullback-Leibler divergence in its loss function.\n\n2. `.from_pretrained()`: This is a method that loads a pre-trained model from a specified source.\n\n3. `\"madebyollin/sdxl-vae-fp16-fix\"`: This is the identifier for a specific pre-trained model. It's likely a version of the Stable Diffusion XL VAE that has been modified to work with 16-bit floating point precision (FP16).\n\n4. `torch_dtype=torch.float16`: This parameter specifies that the model should use 16-bit floating point precision. This is a form of model quantization that reduces memory usage and can speed up computation, especially on GPU hardware that supports it.\n\nThe VAE in this context is probably being used as part of the Stable Diffusion XL pipeline. In image generation models like Stable Diffusion, the VAE is typically used to:\n\n1. Encode input images into a latent space representation.\n2. Decode latent space representations back into images.\n\nUsing a pre-trained VAE can help improve the quality and consistency of generated images, especially when fine-tuning or using the model for specific tasks.\n\nThe use of FP16 precision suggests that this setup is optimized for efficiency, possibly to run on consumer-grade GPUs or to handle larger batch sizes.","metadata":{}},{"cell_type":"code","source":"# example 0: moustache\npipe = StableDiffusionXLPipeline.from_pretrained(\n        CFG.model, \n         vae=vae,\n    torch_dtype=torch.float16, \n    variant=\"fp16\", use_safetensors=True).to(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2024-09-06T13:50:45.214047Z","iopub.execute_input":"2024-09-06T13:50:45.214591Z","iopub.status.idle":"2024-09-06T13:53:55.446924Z","shell.execute_reply.started":"2024-09-06T13:50:45.214550Z","shell.execute_reply":"2024-09-06T13:53:55.445584Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This code is setting up a Stable Diffusion XL pipeline for image generation. Let's break it down:\n\n```python\npipe = StableDiffusionXLPipeline.from_pretrained(\n    CFG.model, \n    vae=vae,\n    torch_dtype=torch.float16, \n    variant=\"fp16\", \n    use_safetensors=True\n).to(\"cuda\")\n```\n\n1. `StableDiffusionXLPipeline`: This is the main class for the Stable Diffusion XL model pipeline, which we imported earlier from the `diffusers` library.\n\n2. `.from_pretrained()`: This method loads a pre-trained model and its components.\n\n3. `CFG.model`: This suggests there's a configuration object (`CFG`) that specifies which pre-trained model to use. The exact model isn't shown in this snippet, but it's likely a path or identifier for a specific Stable Diffusion XL model.\n\n4. `vae=vae`: This is passing the VAE (Variational Autoencoder) that we initialized in the previous code snippet. It's using the custom VAE instead of the default one that comes with the model.\n\n5. `torch_dtype=torch.float16`: This sets the model to use 16-bit floating point precision, which can save memory and potentially speed up computations.\n\n6. `variant=\"fp16\"`: This specifies that we're using the 16-bit floating point variant of the model weights.\n\n7. `use_safetensors=True`: This indicates that the model should use the `safetensors` format for loading weights, which can be faster and more secure than traditional PyTorch serialization.\n\n8. `.to(\"cuda\")`: This moves the entire pipeline to the GPU (assuming CUDA is available), which will significantly speed up computations.\n\nThe comment \"# example 0: moustache\" suggests that this setup is being used for generating images with moustaches, or perhaps adding moustaches to existing images.\n\nThis pipeline setup allows for efficient, GPU-accelerated image generation using the Stable Diffusion XL model, optimized for memory usage with 16-bit precision. It's ready to generate high-quality images based on text prompts or other inputs, depending on how it's used in the subsequent code.","metadata":{}},{"cell_type":"code","source":"\nprompt = \"High-definition, cinematic, close-up photograph of a man\"\nimages = pipe(prompt=prompt, num_inference_steps = CFG.infsteps, \n              num_images_per_prompt = 4)\nimage_grid(images.images, 2, 2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n1. Setting the prompt:\n   ```python\n   prompt = \"High-definition, cinematic, close-up photograph of a man\"\n   ```\n   This line defines the text description (prompt) for the image(s) to be generated. The Stable Diffusion model will use this description to create images that match this description as closely as possible.\n\n2. Generating images:\n   ```python\n   images = pipe(prompt=prompt, num_inference_steps=CFG.infsteps, \n                 num_images_per_prompt=4)\n   ```\n   Here, we're calling the Stable Diffusion XL pipeline (`pipe`) to generate images:\n   - `prompt=prompt` passes our text description to the model.\n   - `num_inference_steps=CFG.infsteps` sets the number of denoising steps. This value is coming from a configuration object (CFG). More steps generally result in higher quality images but take longer to generate.\n   - `num_images_per_prompt=4` tells the pipeline to generate 4 different images for this single prompt.\n\n3. Displaying the images:\n   ```python\n   image_grid(images.images, 2, 2)\n   ```\n   This line calls the `image_grid` function (which we saw defined earlier) to arrange the generated images in a 2x2 grid:\n   - `images.images` is likely a list or array containing the 4 generated images.\n   - The arguments `2, 2` specify that we want a grid with 2 rows and 2 columns.\n\nThis code is performing the following tasks:\n1. It's instructing the Stable Diffusion XL model to create high-definition, cinematic, close-up photographs of a man.\n2. It's generating 4 different versions of this prompt, allowing for variation in the output.\n3. It's then arranging these 4 images in a 2x2 grid for easy viewing and comparison.\n\nThis approach is common in image generation tasks where you want to see multiple interpretations of the same prompt. It allows you to compare different outputs, choose the best results, or observe the range of images the model can produce from a single description.","metadata":{}},{"cell_type":"code","source":"neg_prompt = \"moustache, beard, facial hair\"\n\nimages = pipe(prompt=prompt, negative_prompt = neg_prompt,\n             num_inference_steps= CFG.infsteps, num_images_per_prompt = 4)\n\nimage_grid(images.images, 2, 2)","metadata":{"execution":{"iopub.status.busy":"2024-09-06T13:57:38.144041Z","iopub.execute_input":"2024-09-06T13:57:38.146049Z","iopub.status.idle":"2024-09-06T14:00:43.895406Z","shell.execute_reply.started":"2024-09-06T13:57:38.146005Z","shell.execute_reply":"2024-09-06T14:00:43.894268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Setting the negative prompt:\n   ```python\n   neg_prompt = \"moustache, beard, facial hair\"\n   ```\n   This defines a negative prompt, which tells the model what to avoid in the generated images.\n\n2. Generating images:\n   ```python\n   images = pipe(prompt=prompt, negative_prompt=neg_prompt,\n                num_inference_steps=25, num_images_per_prompt=4)\n   ```\n   Here, we're calling the Stable Diffusion XL pipeline to generate images:\n   - `prompt=prompt` uses the positive prompt defined earlier (the close-up of a man).\n   - `negative_prompt=neg_prompt` passes our negative prompt to the model.\n   - `num_inference_steps=25` sets the number of denoising steps to 25.\n   - `num_images_per_prompt=4` tells the pipeline to generate 4 different images.\n\n3. Displaying the images:\n   ```python\n   image_grid(images.images, 2, 2)\n   ```\n   This arranges the 4 generated images in a 2x2 grid, just like in the previous example.\n\nThe key difference in this code is the use of a negative prompt. Here's what it does:\n\n1. The positive prompt still asks for \"High-definition, cinematic, close-up photograph of a man\".\n2. The negative prompt tells the model to avoid including \"moustache, beard, facial hair\" in the generated images.\n3. The model will attempt to generate images that match the positive prompt while explicitly avoiding features mentioned in the negative prompt.\n\nThis technique is useful when you want to generate images with specific exclusions. In this case, it's likely trying to generate images of men without any facial hair.","metadata":{}},{"cell_type":"code","source":"# example 1 - prompts from https://x.com/WorldEverett/status/1812540956987592920\n\nprompt = \"Minimalist Scandinavian living room with natural light and wooden furniture\"\nimages = pipe(prompt=prompt, num_inference_steps = 25, num_images_per_prompt = 4)\nimage_grid(images.images, 2, 2)","metadata":{"execution":{"iopub.status.busy":"2024-09-06T14:05:19.208945Z","iopub.execute_input":"2024-09-06T14:05:19.209357Z","iopub.status.idle":"2024-09-06T14:08:28.837797Z","shell.execute_reply.started":"2024-09-06T14:05:19.209292Z","shell.execute_reply":"2024-09-06T14:08:28.836677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"neg_prompt = \"plants, lamps, pillows\"\n\nimage = pipe(prompt=prompt, negative_prompt = neg_prompt,\n             num_inference_steps=25, num_images_per_prompt = 4)\n\nimage_grid(image.images, 2, 2)","metadata":{"execution":{"iopub.status.busy":"2024-09-06T14:08:33.748840Z","iopub.execute_input":"2024-09-06T14:08:33.749579Z","iopub.status.idle":"2024-09-06T14:11:39.429377Z","shell.execute_reply.started":"2024-09-06T14:08:33.749534Z","shell.execute_reply":"2024-09-06T14:11:39.428356Z"},"trusted":true},"execution_count":null,"outputs":[]}]}